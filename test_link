import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parents[1]))

import argparse
import time
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from src.models.link import LinkStatePredictor
from src.config.data import LinkState
from debug.mainrunner import mainrunner
from debug.argparser import build_parser, CommandSpec
from debug.stats import print_statistics
from typing import Dict, List, Tuple
from data.loader import DataLoader, shuffle_and_split

# Confusion matrix computations in the prediction test
from sklearn.metrics import confusion_matrix, classification_report 

terrestrial, air = "Rx0", "Rx1"
def generate_training_data(
    n_samples: int, n_receivers: int=2, rx_types: List[str]=None, seed: int=42
) -> Dict[str,np.ndarray]:
    if rx_types is None: rx_types = [terrestrial, air]

    np.random.seed(seed)

    # Generate random UAV positions and then receiver position
    txp = np.random.uniform(-1000,1000,size=(n_samples,3)).astype(np.float32)
    rxp = np.random.uniform(-1000,1000,size=(n_samples,3)).astype(np.float32)

    # Calculate distance vectors
    dvec = txp - rxp
    rx_type = np.random.choice(rx_types, size=n_samples)

    # Simulate link states based on distance and receiver type
    distances = np.linalg.norm(dvec, axis=1)
    heights = dvec[:,2]

    # Simple heuristic for link state
    link_state = np.zeros(n_samples, dtype=np.int32)
    los_prob = np.exp(-distances / 500) * (1 / (1 + np.exp(-heights / 100)))

    # Terrestrial receiver is more likely to have NLOS
    terrestrial_mask = rx_type == terrestrial
    los_prob[terrestrial_mask] *= 0.5
    link_state = np.where(
        np.random.random(n_samples) < los_prob,LinkState.LOS,LinkState.NLOS
    )
    # Add some 'no - link' cases (far distances or blocked)
    no_link_mask = (distances > 800) | ((heights < -50) & terrestrial_mask)
    link_state[no_link_mask] = LinkState.NO_LINK
    return {'dvec': dvec, 'rx_type': rx_type, 'link_state': link_state}

def build_model(args: argparse.Namespace, load: bool=False) -> LinkStatePredictor:
    model = LinkStatePredictor(
        rx_types=[terrestrial, air], dropout_rate=args.dropout, add_zero_los_frac=args.add_los,
        n_unit_links=tuple(map(int, args.hidden_layers.split(','))), seed=args.seed 
    )
    if load: model.load()
    else: model.build()
    return model


# ============================================================
#       Debugging Testing Methods 
# ============================================================

def test_model_construction(args: argparse.Namespace):
    model = build_model(args)
    model.model.summary()


def test_train_model_on_dataset(args: argparse.Namespace):
    loader = DataLoader()
    data = loader.load(args.dataset)
    dtr, dts = shuffle_and_split(
        data, val_ratio=args.val_ratio, test_ratio=args.test_ratio,
        seed=args.seed
    )
    model = build_model(args, False)
    model.fit(dtr=dtr, dts=dts, epochs=args.epochs, batch_size=args.batch_size)
    model.save()


def test_train_model_on_generated(args: argparse.Namespace):
    data = generate_training_data(
        n_samples=args.samples, rx_types=[terrestrial, air], seed=args.seed
    )
    dtr, dts = shuffle_and_split(
        data, val_ratio=args.val_ratio, test_ratio=args.test_ratio, seed=args.seed
    )
    link_state_count = np.bincount(data['link_state'], minlength=LinkState.N_STATES)
    for state in range(LinkState.N_STATES):
        count = link_state_count[state]
        percentage = 100 * count / len(data['link_state'])
    
    model = build_model(args, False)
    history = model.fit(
        dtr, dts, epochs=args.epochs, batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )


def test_model_prediction(args: argparse.Namespace):
    model = build_model(args, True)

    # print(f"Model expects rx_types: {model.rx_types}")
    # print(f"Encoder categories: {model.rx_type_encoder.categories_ if model.rx_type_encoder else 'No encoder'}")

    data = generate_training_data(
        n_samples=args.samples, rx_types=model.rx_types, seed=args.seed
    )
    dtr, dts = shuffle_and_split(
        data, val_ratio=args.val_ratio, test_ratio=args.test_ratio, seed=args.seed
    )

    # Print the distribution of the link states (Remove later, unnecessary)
    link_state_count = np.bincount(data['link_state'], minlength=LinkState.N_STATES)
    for state in range(LinkState.N_STATES):
        count = link_state_count[state]
        percentage = 100 * count / len(data['link_state'])
        print(f"State {state}: {count} samples ({percentage:.1f}%)")
    
    # continue 
    p = model.predict(dtr['dvec'],dts['rx_type'],batch_size=args.batch_size)
    predictions = np.argmax(p, axis=1)

    













# def test_model_prediction(args: argparse.Namespace):
#     model = build_model(args, True)

#     # Generate test data with the SAME rx_types that the model was trained with
#     data = generate_training_data(
#         n_samples=args.samples, rx_types=model.rx_types, seed=args.seed
#     )
#     dtr, dts = shuffle_and_split(
#         data, val_ratio=args.val_ratio, test_ratio=args.test_ratio, seed=args.seed
#     )
    
#     # Print distribution of link states
#     link_state_count = np.bincount(data['link_state'], minlength=LinkState.N_STATES)
#     for state in range(LinkState.N_STATES):
#         count = link_state_count[state]
#         percentage = 100 * count / len(data['link_state'])
#         print(f"State {state}: {count} samples ({percentage:.1f}%)")

#     predictions_prob = model.predict(
#         dts['dvec'], dts['rx_type'], batch_size=args.batch_size
#     )
#     predictions = np.argmax(predictions_prob, axis=1)
    
#     # FIX: Use dts, not test_data (which doesn't exist)
#     accuracy = np.mean(predictions == dts['link_state'])
#     print(f"Accuracy: {accuracy:.4f}")

#     cm = confusion_matrix(dts['link_state'], predictions)
#     print("Confusion Matrix:")
#     print(cm)
    
#     # Optional: Print classification report
#     print("\nClassification Report:")
#     print(classification_report(dts['link_state'], predictions, 
#                                 target_names=['NO_LINK', 'LOS', 'NLOS']))
    
#     return predictions, dts['link_state']



def test_model_sampling(args: argparse.Namespace):
    test_data = generate_training_data(
        n_samples=args.samples, rx_types=args.rx_types.split(','),
        seed=args.seed + 1  # Different seed from training
    )

    if not hasattr(args, 'model') or args.model is None:
        rx_types = args.rx_types.split(',')
        model = LinkStatePredictor(
            rx_types=rx_types, n_unit_links=(64, 32),
            dropout_rate=0.2, seed=args.seed
        )
        model.build()

        train_data = generate_training_data(1000, rx_types=rx_types, seed=args.seed)
        test_data_small = generate_training_data(200, rx_types=rx_types, seed=args.seed + 2)
        
        model.fit(train_data, test_data_small, epochs=5, batch_size=32)
    else:
        model = args.model
    
    result, stats = benchmark(
        model.sample_link_state,
        test_data['dvec'], test_data['rx_type'],
        batch_size=args.batch_size, repeat=args.repeat, warmup=2
    )
    
    print_benchmark_results(
        stats, n_samples=len(test_data['dvec']), label="Link State Sampling"
    )
    
    # Analyze sampled states distribution
    sampled_states = model.sample_link_state(
        test_data['dvec'], test_data['rx_type'], batch_size=args.batch_size
    )
    
    unique, counts = np.unique(sampled_states, return_counts=True)
    for state, count in zip(unique, counts):
        percentage = 100 * count / len(sampled_states)
        print(f"  State {int(state)} ({LinkState(int(state)).name}): {count} ({percentage:.1f}%)")
    
    return sampled_states



# ============================================================
#       Main Function Building Main Script
# ============================================================

SEED = [{"flags": ["--seed","-s"], "kwargs": {"type": int, "default": 42}}]
DATA = [{"flags": ["dataset"], "kwargs": {"type":str, "default": "uav_london/train.csv"}},]
MODEL_ARGS = [
    {"flags": ["--hidden-layers", "-hl"], "kwargs": {"type": str, "default": "64,32,16"}},
    {"flags": ["--dropout","-d"], "kwargs": {"type": float, "default": 0.2}},
    {"flags": ["--add-los","-al"], "kwargs": {"type": float, "default": 0.1}},
    {"flags": ["--unit-links","-ul"], "kwargs": {"type": str, "default": "25,10"}},
]
TRAIN_ARGS = [
    {"flags": ["--samples","-n"], "kwargs": {"type": int, "default": 1000}},
    {"flags": ["--epochs","-e"], "kwargs": {"type": int, "default": 100}},
    {"flags": ["--val-ratio","-vr"], "kwargs": {"type": float, "default": 0.2}},
    {"flags": ["--test-ratio","-tr"], "kwargs": {"type": float, "default": 0.0}},
    {"flags": ["--learning-rate","-lr"], "kwargs": {"type": float, "default": 1e-3}},
    {"flags": ["--batch-size", "-bs"], "kwargs": {"type": int, "default": 512}}
]

@mainrunner
def main():
    parser = build_parser([
        CommandSpec(
            name="construct", help="Test model construction and architecture",
            handler=test_model_construction, extra_args=[*MODEL_ARGS, *SEED],
        ),
        CommandSpec(
            name="train-load", help="Load the dataset and train the model",
            handler=test_train_model_on_dataset, extra_args=[
                *DATA, *TRAIN_ARGS, *SEED, *MODEL_ARGS
            ]
        ),
        CommandSpec(
            name="train-gen", help="Generate synthetic dataset and train a model on it",
            handler=test_train_model_on_generated, extra_args=[
                *TRAIN_ARGS, *SEED, *MODEL_ARGS 
            ]
        ),
        CommandSpec( # repeat could be added
            name="predict", help="Testing the predictions of the stored model (if exists)",
            handler=test_model_prediction, extra_args=[*SEED,*MODEL_ARGS, *TRAIN_ARGS]
        ),
        CommandSpec(
            name="sample",
            help="Test link state sampling from probabilities",
            handler=test_model_sampling,
            extra_args=[
                {"flags": ["--samples", "-n"], "kwargs": {"type": int, "default": 1000}},
                {"flags": ["--rx-types"], "kwargs": {"type": str, "default": "terrestrial,air-bound"}},
                {"flags": ["--batch-size", "-b"], "kwargs": {"type": int, "default": 512}},
                {"flags": ["--repeat", "-r"], "kwargs": {"type": int, "default": 5}},
                {"flags": ["--seed", "-s"], "kwargs": {"type": int, "default": 42}}
            ]
        ),
    ])

    args = parser.parse_args()
    args._handler(args)


if __name__ == "__main__":
    main()
